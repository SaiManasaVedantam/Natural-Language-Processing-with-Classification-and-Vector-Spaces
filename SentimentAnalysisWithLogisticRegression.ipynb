{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysisWithLogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis with Logistic Regression:\n",
        "1. For a given text, extract features for Logistic Regression\n",
        "2. Implement Logistic Regression\n",
        "3. Apply Logistic Regression on an NLP task\n",
        "4. Test and perform error analysis\n",
        "\n",
        "Learned and implemented as a part of NLP course on Coursera"
      ],
      "metadata": {
        "id": "zAHPufzkruTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "import re\n",
        "import math\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "f3isAxTosQIW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get sample tweets data & stopwords\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rReY9nArtGaf",
        "outputId": "a408ac02-4efe-43a9-df9e-0d7d8d996f71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to process a given tweet to be used to train with Logistic Regression"
      ],
      "metadata": {
        "id": "CZB43WNNtRln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: A string (tweet)\n",
        "# Output: A list of words containing the processed tweet\n",
        "\n",
        "def process_tweet(tweet):\n",
        "\n",
        "  stemmer = PorterStemmer()\n",
        "  stopwords_english = stopwords.words('english')\n",
        "\n",
        "  # Remove stock market tickers like $GE\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "\n",
        "  # Remove retweet text \"RT\"\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "  # Remove hyperlinks\n",
        "  tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "\n",
        "  # Remove hash sign\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "  # Tokenize tweets\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  tweets_clean = []\n",
        "  for word in tweet_tokens:\n",
        "    if word not in stopwords_english and word not in string.punctuation:  \n",
        "      # Remove stopwords & punctuation\n",
        "      # Obtain word stem\n",
        "      stem_word = stemmer.stem(word)\n",
        "      tweets_clean.append(stem_word)\n",
        "\n",
        "  return tweets_clean\n"
      ],
      "metadata": {
        "id": "trupoR0lsmPa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to build the frequency map"
      ],
      "metadata": {
        "id": "l9cJRE7Lu_LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: List of tweets and their corresponding labels (0 or 1)\n",
        "# Output: A frequency map of the form <(word, sentiment) : count>\n",
        "\n",
        "def build_freqs(tweets, labels):\n",
        "  # Convert np array to list since zip needs an iterable\n",
        "  # The squeeze is necessary or the list ends up with one element\n",
        "  # Also note that this is just a NOP if labels is already a list\n",
        "  labels_list = np.squeeze(labels).tolist()\n",
        "\n",
        "  freqs = {}\n",
        "  for label, tweet in zip(labels_list, tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word, label)\n",
        "      if pair in freqs:\n",
        "        freqs[pair] += 1\n",
        "      else:\n",
        "        freqs[pair] = 1\n",
        "\n",
        "  return freqs\n",
        "  "
      ],
      "metadata": {
        "id": "SE87XBf1texh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to generate Sigmoid"
      ],
      "metadata": {
        "id": "EX3l9VDG7gm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: z = ùúÉ0ùë•0+ùúÉ1ùë•1+ùúÉ2ùë•2+...ùúÉùëÅùë•ùëÅ, may be a scalar or an array\n",
        "# Output: h = sigmoid of z\n",
        "\n",
        "def sigmoid(z): \n",
        "\n",
        "  h = None\n",
        "  z_type = type(z)\n",
        "  \n",
        "  # 1. Assuming z is scalar, we return a scalar\n",
        "  if z_type == int or z_type == float:\n",
        "    h = 1 / (1 + math.exp(-1 * z))\n",
        "  \n",
        "  # 2. Assuming z is an array: Then, h is also an array of sigmoid values\n",
        "  else:\n",
        "    h = []\n",
        "    for each_z in z:\n",
        "      sigmoid_value = 1 / (1 + math.exp(-1 * each_z))\n",
        "      h.append(sigmoid_value)\n",
        "    h = np.array(h)\n",
        "  \n",
        "  return h\n"
      ],
      "metadata": {
        "id": "csAPBfWW7djQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to implement gradient descent"
      ],
      "metadata": {
        "id": "To_MF1N56KG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input:\n",
        "# x: matrix of features which is (m,n+1)\n",
        "# y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "# theta: weight vector of dimension (n+1,1)\n",
        "# alpha: learning rate\n",
        "# num_iters: number of iterations you want to train your model for\n",
        "\n",
        "# Output:\n",
        "# J: Final cost\n",
        "# theta: Final weight vector\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "  \n",
        "  m = len(x)\n",
        "  for i in range(0, num_iters):\n",
        "    z = x @ theta\n",
        "    h = sigmoid(z)\n",
        "    \n",
        "    # Calculate the cost function\n",
        "    log_h = []\n",
        "    log_1_minus_h = []\n",
        "    for each_h in h:\n",
        "      log_h.append(math.log(each_h))    \n",
        "      log_1_minus_h.append(math.log(1 - each_h))\n",
        "    \n",
        "    ones_matrix_of_size_mx1 = np.ones((m, 1))\n",
        "    one_minus_y = ones_matrix_of_size_mx1 - y\n",
        "    \n",
        "    # Compute cost: J\n",
        "    J = (-1 / m) * ( (np.transpose(y) @ np.array(log_h)) + (np.transpose(one_minus_y) @ np.array(log_1_minus_h)) )\n",
        "\n",
        "    # Update the weights theta\n",
        "    theta = theta - ( (alpha / m) * (np.transpose(x) @ (np.transpose(np.matrix(h)) - y)) )\n",
        "      \n",
        "  J = float(J)\n",
        "  theta = np.array(theta)\n",
        "  \n",
        "  return J, theta\n"
      ],
      "metadata": {
        "id": "jqlUxFud54Jp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract features for Logistic Regression"
      ],
      "metadata": {
        "id": "Lj640IT660qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: \n",
        "# tweet: a list of words for one tweet\n",
        "# freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "\n",
        "# Output: x = A feature vector of dimension (1,3)\n",
        "\n",
        "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
        "    \n",
        "  # process_tweet tokenizes, stems, and removes stopwords\n",
        "  word_l = process_tweet(tweet)\n",
        "  \n",
        "  # 3 elements in the form of a 1 x 3 vector\n",
        "  x = np.zeros((1, 3)) \n",
        "  \n",
        "  # Bias term is set to 1\n",
        "  x[0,0] = 1 \n",
        "  \n",
        "  # Loop through each word in the list of words\n",
        "  for word in word_l:\n",
        "      \n",
        "    # Increment the word count for the positive label 1\n",
        "    x[0,1] += freqs[(word, 1)] if (word, 1) in freqs.keys() else 0\n",
        "    \n",
        "    # Increment the word count for the negative label 0\n",
        "    x[0,2] += freqs[(word, 0)] if (word, 0) in freqs.keys() else 0\n",
        "      \n",
        "  assert(x.shape == (1, 3))\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "y23jgOHR61Pg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to predict sentiment of a tweet"
      ],
      "metadata": {
        "id": "V_1DUDvc7stb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: \n",
        "# tweet: a string\n",
        "# freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "# theta: (3,1) vector of weights\n",
        "\n",
        "# Output: y_pred = the probability of a tweet being positive or negative\n",
        "\n",
        "def predict_tweet(tweet, freqs, theta):\n",
        "    \n",
        "  # Extract the features of the tweet and store it into x\n",
        "  x = extract_features(tweet, freqs)\n",
        "  \n",
        "  # Make the prediction using x and theta\n",
        "  logits = x @ theta\n",
        "  y_pred = np.matrix(sigmoid(logits))\n",
        "\n",
        "  return y_pred\n"
      ],
      "metadata": {
        "id": "tOuSYM7-7wFG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to test Logistic Regression"
      ],
      "metadata": {
        "id": "aRjNvGKC792O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: \n",
        "# test_x: a list of tweets\n",
        "# test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "# freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "# theta: weight vector of dimension (3, 1)\n",
        "\n",
        "# Output: accuracy = (# of tweets classified correctly) / (total # of tweets)\n",
        "\n",
        "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
        "    \n",
        "  # List for storing predictions\n",
        "  y_hat = []\n",
        "  \n",
        "  for tweet in test_x:\n",
        "    # Get the label prediction for the tweet\n",
        "    y_pred = predict_tweet(tweet, freqs, theta)\n",
        "    y_pred = y_pred[0]\n",
        "    \n",
        "    if y_pred > 0.5:\n",
        "      # Append 1.0 to the list\n",
        "      y_hat.append(1.0)\n",
        "    else:\n",
        "      # Append 0 to the list\n",
        "      y_hat.append(0.0)\n",
        "\n",
        "  # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "  # Convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "  y_hat = np.array(y_hat)\n",
        "  correct, m = 0, len(test_x)\n",
        "\n",
        "  for idx in range(0, m):\n",
        "    correct += 1 if y_hat[idx] == test_y[idx] else 0\n",
        "  accuracy = np.float64(np.array([correct / m]))\n",
        "\n",
        "  return accuracy\n"
      ],
      "metadata": {
        "id": "2yGeghRy74Wl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# Split the data into two pieces, one for training and one for testing (validation set) \n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg \n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# Combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
        "\n",
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))\n",
        "\n",
        "# Create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "# Check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mKd1juQ-Zcx",
        "outputId": "2e66f246-b453-49a6-eb25-2b843d360669"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n",
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "  X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# Training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nO5lQhD-q6D",
        "outputId": "53bead01-4cb8-498a-a44b-555e06afffec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.22521260.\n",
            "The resulting vector of weights is [6e-08, 0.0005382, -0.0005583]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Error analysis\n",
        "print('Label Predicted Tweet')\n",
        "for x,y in zip(test_x,test_y):\n",
        "  y_hat = predict_tweet(x, freqs, theta)\n",
        "\n",
        "  if np.abs(y - (y_hat > 0.5)) > 0:\n",
        "    print('THE TWEET IS:', x)\n",
        "    print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
        "    print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))\n",
        "\n",
        "# Prediction\n",
        "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
        "print(process_tweet(my_tweet))\n",
        "\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "\n",
        "if y_hat > 0.5:\n",
        "  print('Positive sentiment')\n",
        "else: \n",
        "  print('Negative sentiment')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lixvbeL8MlO",
        "outputId": "a7665a3e-c410-4a06-e795-19c65c23fdc3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Predicted Tweet\n",
            "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
            "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
            "1\t0.48942982\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
            "http://t.co/UGQzOx0huu\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48418982\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48418982\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48418982\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: off to the park to get some sunlight : )\n",
            "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
            "1\t0.49636406\tb'park get sunlight'\n",
            "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
            "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
            "1\t0.48250522\tb'uff itna miss karhi thi ap :p'\n",
            "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
            "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
            "0\t0.50988296\tb'u prob fun david'\n",
            "THE TWEET IS: pats jay : (\n",
            "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
            "0\t0.50040366\tb'pat jay'\n",
            "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
            "0\t0.50000002\tb'belov grandmoth'\n",
            "THE TWEET IS: Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n",
            "THE PROCESSED TWEET IS: ['sr', 'financi', 'analyst', 'expedia', 'inc', 'bellevu', 'wa', 'financ', 'expediajob', 'job', 'job', 'hire']\n",
            "0\t0.50648699\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n",
            "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
            "[[0.48125421]]\n",
            "Negative sentiment\n"
          ]
        }
      ]
    }
  ]
}